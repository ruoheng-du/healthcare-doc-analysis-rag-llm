# Chunking Module

This module provides multiple ways to split parsed documents into smaller chunks
for embeddings, retrieval, or LLM input.  
All methods read parsed JSON from and write results under
the processed and chunked directory defined in `configs/ingest_parse.yaml`.

---

## Methods

### 1. Fixed-Size (`fix_size_chunking.py`)
- Splits text into fixed-size character chunks.
- Tracks page numbers spanned.
- **Params:** `chunk_size`, `overlap` (optional).
- **Output:** `data/chunks/fix_size_chunking_result`.

### 2. Asterisk-Separated (`pattern_chunking.py`)
- Splits when a delimiter (default: â‰¥10 `*`) is found or when max size is hit.
- Delimiter is removed in output.
- **Params:** `delimiter_pattern`, `min_repeat`, `max_chunk_size`.
- **Output:** `data/chunks/asterisk_chunking_result`.

### 3. Semantic (`semantic_chunking.py`)
- Uses embeddings to split at low-similarity points (sentence/paragraph level).
- Default model: [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).
- **Params:** `model_name`, `unit`, `similarity_threshold`, `max_chunk_size`.
- **Output:** `data/chunks/semantic_chunking_result`.

---

## Input / Output

- **Input:** JSON files with `full_text` and `pages` from `data/processed/`.
- **Output:** JSONL per method, one file per input doc.  
  Example entry:

```json
{
  "chunk_id": "file_0",
  "text": "chunk text ...",
  "pages": [1,2],
  "metadata": {
    "source_file": "data/processed/example.json",
    "chunk_index": 0,
    "similarity_score": 0.78 (only for semantic chunking)
  }
}
```
Configuration
Paths are set in configs/ingest_parse.yaml:

yaml
Copy code
paths:
  raw: data/raw/...
  processed: data/processed
  chunked: data/chunks
Notes
Errors in a file (e.g. missing full_text) are logged and skipped.

Semantic chunking requires:

sentence-transformers (embeddings)

nltk with resources punkt and punkt_tab (sentence tokenization)
